%2025 - 4c
\section{04c Annexe : Algèbre multilinéaire}

Objectif : 1. se mettre d'accord sur les notations, 2. rappels sur les espaces vectoriel, 3. un tenseur est une application multilinéaire.


\subsection{Espaces vectoriels}

(V, $+$,$\cdot$) est un espace vectoriel. Par définition, $+$ et $\cdot$ respecte les axiomes (CANI ADDU) :

L'opération $+$, est Commutative, Associative, il y a un vecteur Nul et tout vecteur possède un Inverse.

L'opération $\cdot$ est :

 - Associative
$\mu\cdot(\lambda\cdot\,\mv{v}\ )=(\mu.\lambda)\cdot\,\mv{v}$
{ \it il faut distinguer ici la multiplication dans $\mb{R}$ "." de la multiplication dans V "$\cdot$"}

 - Distributive :
$(\lambda + \mu)\cdot\,\mv{v} = \lambda\cdot\,\mv{v} + \mu\cdot\,\mv{v}$


 - Distributive :
 $\lambda\cdot(\,\mv{v}+\,\mv{w}\ ) = \lambda\cdot\mv{v}\, + \lambda\cdot\,\mv{w}$

 - Unité : $1\cdot\,\mv{v} = \mv{v}$

\subsection{Base et dimension}

En dimension fini, il existe une base $\{\,\mv{e}_{\ (1)},\ \mv{e}_{\ (2)},\,...\,,\ \mv{e}_{\ (n)} \}$ tel que quelquesoit $\ \mv{v}\ $ il existe un unique $(v^1,...,v^n)$ tel que $\ \mv{v}\ =\sum \ v^{\,i\,}\ \mv{e}_{\ (i)}$. Autrement dit,

\[
\exists \{\,\mv{e}_{\ (i)}\}_{i=1,...,n}\ tq\ \forall\ \mv{v}\ \in V, \exists!\ (v^1,...,v^n)\ tq\ \ \mv{v}\ =\sum_{i=1}^n \ v^{\,i\,}\ \mv{e}_{\ (i)}
\]

$v^{\,i\,}$ est la i-ème composante, c'est un scalaire, $\mv{e}_{\ (i)}$ est le i-ème vecteur de la base. Les parenthèses autour de l'indice précise qu'il ne s'agit pas d'une coordonnées mais d'un vecteur. On peut changer de base : $\mr{B'}\{\ \mv{e'}_{\ (1)},\ \mv{e'}_{\ (2)},\,...\,,\ \mv{e'}_{\ (n)} \}$, les $\ \mv{e'}_{\ (i)}$ peuvent s'exprimer en fonction des $\ \mv{e}_{\ (1)}$ :

\[
\mv{e'}_{\ (i)}=\sum_{j=1}^n \ P_i^{\,j}\ \ \mv{e}_{\ (j)}
\]
On remarque que la somme se fait sur un indice apparaissant une fois en haut et une fois en bas, et que l'indice apparaissant en bas dans le terme de gauche gauche, apparaît en bas dans le terme de droite. Ceci est général et permet d'éviter les erreurs (nous justifierons plus tard la convention d'Einstein).

$P_i^j$ est la matrice de passage. On peut alors exprimer les coordonnées d'un vecteur dans la nouvelle base :
\[
\mv{v}\ =\sum_{i=1}^n \ v'^{\,i\,}\ \mv{e'}_{(i)} = 
\sum_{i=1}^n \ v'^{\,i\,}\ \sum_{j=1}^n \ P_i^{\,j}\ \ \mv{e}_{\ (j)} = 
\sum_{j=1}^n\ \sum_{i=1}^n\ P_i^{\,j}\ v'^{\,i\,} \ \ \mv{e}_{\ (j)}
\]
d'où
\[
 v^{\,i} = \sum_{j=1}^n P_j^{\,i} v'^j
\]

\subsection{Composantes contravariantes}

\begin{minipage}[c]{.55\linewidth}
\hspace{0.5cm} La même matrice $P_i^j$ permet de passer de la base $\mr{B}$ à la base $\mr{B'}$ et des coordonnées $v'^{\,i}$ aux coordonnées $v^i$. Cela est naturel : si on multiplie les vecteurs de base par 3, les coordonnées d'un vecteur seront divisées par 3.
\end{minipage}
\hfill
\begin{minipage}[c]{.35\linewidth}
%\begin{center}
\begin{tikzpicture}
\def\largeur{3.1} \def\hauteur{1.3} \def\decalage{0.4}
\node (A) at (0,\hauteur) {$\mr{B}\ $};
\node (B) at (\largeur,\hauteur) {$\ \mr{B'}$};
\node (C) at (0,0) {$v^i\ $};
\node (D) at (\largeur,0) {$\ v'^{\,i}$};
\node (E) at (0.5*\largeur ,\decalage) {$P_i^j$};
\node (F) at (0.5*\largeur ,\decalage + \hauteur) {$P_i^j$};
\draw[very thick,->] (A)--(B); \draw[very thick,->] (D)--(C);
\end{tikzpicture}
%\end{center}
\end{minipage}
C'est pour cela qu'on appelle contravariante les composantes des vecteurs, elles varient à l'inverse des vecteurs de la base.
\[
[P^{\,i}_j]^{-1} \neq [P^{\,j}_i] \qquad et \qquad V = P V'\ \to\ V'=P'V
\]

\subsection{Fonctions linéaires}
Une application f d'un espace vectoriel vers un autre espace vectorielle
\[
f : (V, +,\cdot) \to (W, +,\cdot)
\]
est linéaire si
\[
\forall\ (\ \mv{u}\ ,\ \mv{v}\ )\in V,\forall\ \lambda\in\mb{R},
\qquad f(\ \mv{u}\ +\lambda\cdot\ \mv{v}\ )=f(\ \mv{u}\ )+\lambda\cdot f(\ \mv{v}\ )
\]

On note Hom (d'homomorphisme) l'ensemble des applications linéaire :
\[
Hom = \{applications\ linéaire\ de V\ dans\ W\}
\]
c'est un espace vectoriel si on le munie d'une addition et d'une multiplication scalaire "point par point", c'est à dire :

\[
\forall\ \mv{v}\ ,\qquad( f_1 +\lambda f_2 ) v = ( f_1(v) +\lambda f_2(v) )
\]
on passe de $(+,\dot)$ de Hom à $(+,\dot)$ de W.

Remarque : une application linéaire f est entièrement caractérisée si on connaît l'image des vecteurs d'une base de V par f :
\[
Soit\ une\ base\ de\ V\ : \quad \mr{B}\{\ \mv{e}_{\ (i)} \}_{i=1,...,dim V}
\]
alors
\[
\forall\ \ \mv{v}\ \in V,\ \ \exists v^1, v^2,...\ ,v^n \ \ 
\ \mv{v}\ =\sum_i v^i\ \ \mv{e}_{\ (i)} \
\]
et
\[
f(\ \mv{v}\ )\ = f(\sum_i v^i\ \ \mv{e}_{\ (i)} \ )
= \sum_i v^i\ f(\ \mv{e}_{\ (i)} \ )
\]


Donc, si l'on connaît l'image de n éléments, par linéarité, on connaît l'image de tous les éléments.

Or $ f(\ \mv{e}_{\ (i)} \ ) \in W$, on peut donc l'écrire dans une base de W, $\mr{B}_W\{\ \mv{f}_{\ (i)} \}$ :

\[
f(\ \mv{e}_{\ (i)} \ ) = \sum_{j=1}^{dim\ W} F^j_i\ \ \mv{h}_{\ (j)}
\]
et on obtient alors
\[
f(\ \mv{v}\ )\ =
\sum_i v^i\ \sum_j F^j_i\ \ \mv{h}_{\ (j)} =
\sum_j \sum_i v^i\ F^j_i\ \ \mv{h}_{\ (j)} \equiv \ \mv{w}
\]
Soit
\[
(v^1,...,v^{dim V}) \in V\ \to
\ (w^1,...,w^{dim W}) \in W
\]
et finallement
\[
w^j = \sum_i F^j_iv^i
\]
$F^j_i$ est une matrice (dim W $\times$ dim V), on en déduit la dimension de Hom :
\[
dim\ Hom(V,W) = dim V . dim W
\]

\subsection{Espace vectoriel dual, V*}
Le dual de V , noté V* est Hom(V,$\mb{R}$), est l'ensemble des formes
linéaires ( = application linéaire de V dans $\mb{R}$ ). C'est un espace
vectoriel de même dimension que V (en dimension fini, mais l'espace-temps
est de dimension fini, à priori).
\subsection{Base duale d'une base donnée}
Si $\{\ \mv{e}_{\ (i)} \}_{i=1,...,dim V}$ est une base de
(V, $+$, $\cdot$) la base dual de ($V^*$,$+$, $\cdot$) est la famille de
forme linéaire $\{\ \vd{\varepsilon}^{\ (i)}\}_{i=1,...,n}$ tel que

\[
\forall i, \forall i, \qquad \vd{\varepsilon}^{\ (i)}(\ \mv{e}_{\ (i)} )
= \delta^i_j
\]

Une forme linéaire de $V^*$ est entièrement défini si on connaît l'image des $\ \mv{e}_{\ (i)}$

\[
\forall\ \vd{\varrho}\ \in V^*, \qquad \ \vd{\varrho}\ =\sum_i
\left[ \ \vd{\varrho}\ (\ \mv{e}_{\ (i)} )\right] \vd{\varepsilon}^{\ (i)}
\equiv \rho_i \ \vd{\varepsilon}^{\ (i)}
\]

